# Validation Framework - Quick Start Guide

## What Is This?

A comprehensive **publication-grade validation framework** for your GNN-based Knapsack solver. It automates statistical testing, baseline comparisons, and generates publication-ready outputs (LaTeX tables, high-quality figures).

## One-Liner

```bash
knapsack-gnn validate --checkpoint checkpoints/run_20251020_104533 --check-power --latex --figures
```

That's it! This runs complete validation and generates everything you need for your paper.

## What You Get

After running validation (2-5 minutes), you'll have:

```
validation_report/
â”œâ”€â”€ validation_results.json              # All results
â”œâ”€â”€ validation_report.txt                # Summary
â”œâ”€â”€ baseline_comparison_table.tex        # ðŸ“„ Copy-paste into paper
â”œâ”€â”€ statistical_tests_table.tex          # ðŸ“Š Statistical tests
â”œâ”€â”€ method_comparison.pdf                # ðŸ“ˆ Publication figure
â””â”€â”€ confidence_intervals.pdf             # ðŸ“Š CI plot
```

## Example Output

### LaTeX Table (Ready for Paper)

```latex
\begin{table}[t]
\caption{Performance comparison on 200 test instances}
\label{tab:baseline_comparison}
\begin{tabular}{lccc}
\toprule
Method & Mean Gap (\%) & Median Gap (\%) & Feasibility \\
\midrule
GNN-PNA & \textbf{0.07} & \textbf{0.00} & \textbf{100.00\%} \\
Greedy & 0.49 & 0.13 & 100.00\% \\
Random & 11.47 & 12.67 & 100.00\% \\
\bottomrule
\end{tabular}
\end{table}
```

### Statistical Summary

```
STATISTICAL COMPARISON
======================================================================
GNN-PNA vs Greedy:
  Paired t-test: p < 0.001 â­ SIGNIFICANT
  95% CI: [0.32%, 0.66%]
  Cohen's d = 1.23 (large effect)

â†’ GNN significantly outperforms Greedy with large practical effect
```

## Common Use Cases

### 1. Quick Validation (Before Submission)

```bash
# Validate your best model
knapsack-gnn validate --checkpoint checkpoints/run_XXX
```

**Time**: 2-5 minutes  
**Output**: Baseline comparisons, statistical tests, LaTeX tables

### 2. Full Validation (For Journal Paper)

```bash
# Complete validation with cross-validation
knapsack-gnn validate \
  --checkpoint checkpoints/run_XXX \
  --run-cv \
  --cv-folds 5 \
  --stratify-cv \
  --check-power \
  --output-dir validation_full
```

**Time**: 30-60 minutes  
**Output**: Everything + cross-validation + power analysis

### 3. Custom Baselines

```bash
# Compare against specific baselines
knapsack-gnn validate \
  --checkpoint checkpoints/run_XXX \
  --baselines greedy \
  --baselines random
```

## What Tests Are Run?

### Automatically Performed

1. **Paired t-test**: Is the difference statistically significant?
2. **Wilcoxon test**: Non-parametric alternative (if data non-normal)
3. **Effect sizes**: How big is the improvement? (Cohen's d)
4. **Confidence intervals**: What's the uncertainty? (Bootstrap CI)
5. **Assumption checking**: Are parametric tests valid?

### Optional (with flags)

6. **Cross-validation** (`--run-cv`): How well does it generalize?
7. **Power analysis** (`--check-power`): Is sample size adequate?
8. **Multiple comparison** (automatic if 3+ methods): Overall difference test

## Writing Results for Your Paper

### Template

Copy this structure for your Results section:

```
The GNN-PNA model achieved a mean optimality gap of X% Â± Y% (M Â± SD),
significantly outperforming the Greedy baseline (Z% Â± W%, t = A.BC,
p < 0.001, d = D.EF [large effect]). Five-fold cross-validation
yielded consistent performance (CV gap: X'% Â± Y'%, 95% CI: [L%, U%]).
```

### Example (Using Your Data)

```
The GNN-PNA model achieved a mean optimality gap of 0.07% Â± 0.34%,
significantly outperforming the Greedy baseline (0.49% Â± 1.23%,
t(199) = 8.45, p < 0.001, d = 1.23 [large effect]). The model
maintained 100% feasibility across all test instances.
```

## Features

### Statistical Tests

- âœ… Paired t-test (parametric)
- âœ… Wilcoxon signed-rank (non-parametric)
- âœ… Mann-Whitney U (independent groups)
- âœ… Friedman test (3+ methods)
- âœ… Sign test (robust alternative)

### Effect Sizes

- âœ… Cohen's d (standardized difference)
- âœ… Cliff's Delta (non-parametric)
- âœ… Vargha-Delaney A (probability)

### Corrections

- âœ… Bonferroni (conservative)
- âœ… Holm-Bonferroni (less conservative)
- âœ… Benjamini-Hochberg (FDR control)

### Validation

- âœ… K-fold cross-validation
- âœ… Stratified CV (by problem size)
- âœ… Leave-one-size-out (extreme OOD)
- âœ… Power analysis

## Installation

No extra installation needed! The framework is already integrated into your project.

Optional (for power analysis):
```bash
pip install statsmodels
```

## Configuration

Use a config file for reproducibility:

```yaml
# experiments/configs/validation_config.yaml
statistical:
  alpha: 0.05
  n_bootstrap: 10000

baselines:
  - greedy
  - random

output:
  generate_latex: true
  generate_figures: true
  figure_dpi: 300
```

Then run:
```bash
knapsack-gnn validate \
  --checkpoint checkpoints/run_XXX \
  --config experiments/configs/validation_config.yaml
```

## Interpreting Results

### P-values

- `p < 0.05`: Statistically significant â­
- `p < 0.01`: Highly significant â­â­
- `p < 0.001`: Very highly significant â­â­â­
- `p â‰¥ 0.05`: Not significant

### Effect Sizes (Cohen's d)

- `|d| < 0.2`: Negligible
- `0.2 â‰¤ |d| < 0.5`: Small
- `0.5 â‰¤ |d| < 0.8`: Medium
- `|d| â‰¥ 0.8`: Large

### Confidence Intervals

- **Narrow CI**: Precise estimate
- **Wide CI**: High uncertainty
- **CI excludes 0**: Significant difference
- **CI includes 0**: No significant difference

## Troubleshooting

### "statsmodels not installed"

```bash
pip install statsmodels
```

### "Normality assumption violated"

âœ… **No problem!** The framework automatically uses non-parametric tests (Wilcoxon) when normality is violated. Check the report for recommendations.

### "Sample size too small"

- Use descriptive statistics
- Report effect sizes
- Acknowledge limitation in paper

## Full Documentation

For complete details, see:
- **User Guide**: `docs/VALIDATION_FRAMEWORK.md`
- **Implementation Summary**: `VALIDATION_IMPLEMENTATION_SUMMARY.md`
- **Config Template**: `experiments/configs/validation_config.yaml`

## Command Reference

```bash
# Minimal (fastest)
knapsack-gnn validate --checkpoint <path>

# Recommended (balanced)
knapsack-gnn validate --checkpoint <path> --check-power --latex --figures

# Complete (for journal)
knapsack-gnn validate --checkpoint <path> \
  --run-cv --cv-folds 5 --stratify-cv \
  --check-power --latex --figures

# Custom output directory
knapsack-gnn validate --checkpoint <path> --output-dir my_validation

# Specific baselines
knapsack-gnn validate --checkpoint <path> --baselines greedy

# See all options
knapsack-gnn validate --help
```

## Academic Standards Met

This framework helps meet requirements from:

- âœ… **NeurIPS/ICML**: Statistical rigor, effect sizes, CI
- âœ… **JMLR**: Power analysis, cross-validation
- âœ… **IEEE/ACM**: Publication-quality figures
- âœ… **OR Journals**: Exact solver comparisons, gap analysis

## Questions?

1. Read `docs/VALIDATION_FRAMEWORK.md` (complete guide)
2. Check examples in `experiments/pipelines/publication_validation.py`
3. Review module docstrings
4. Open GitHub issue

---

**TL;DR**: Run `knapsack-gnn validate --checkpoint <your_checkpoint>` and get publication-ready statistical validation! ðŸŽ‰
