# Results - Publication Snapshots

**Last updated:** 2025-10-22

This directory contains **curated, publication-ready results** from experiments.

## ğŸ“ Purpose

Unlike `checkpoints/` (which contains raw outputs from every run), `results/` contains:
- âœ… **Canonical snapshots** that are referenced in papers/docs
- âœ… **Baseline comparisons** for README figures
- âœ… **Publication-quality plots** (300 DPI PNG)
- âœ… **Summary tables** (CSV, JSON) for reference

## ğŸ¯ What Belongs Here

**DO put here:**
- Figures referenced in README.md or documentation
- Baseline comparison results (e.g., GNN vs Greedy vs Random)
- Final validation metrics from published experiments
- Summary tables for paper submissions
- Any result you want to cite/reference later

**DON'T put here:**
- Raw outputs from individual training runs (â†’ `checkpoints/<run_id>/`)
- Intermediate debugging results
- Temporary experiment outputs
- Large binary files (â†’ Git LFS if truly needed)

## ğŸ“‚ Current Structure

```
results/
â”œâ”€â”€ ablations/
â”‚   â”œâ”€â”€ architecture/                       # Architecture comparisons (PNA/GCN/GAT)
â”‚   â”‚   â”œâ”€â”€ architecture_ablation.json      # Aggregate metrics table
â”‚   â”‚   â”œâ”€â”€ architecture_ablation.png       # Bar chart used in docs
â”‚   â”‚   â”œâ”€â”€ learning_curves.png             # Overlayed learning curves
â”‚   â”‚   â””â”€â”€ {gat,gcn,pna}/                  # Per-arch training curves + histories
â”‚   â””â”€â”€ features/
â”‚       â”œâ”€â”€ feature_ablation.json           # Summary of feature drop tests
â”‚       â””â”€â”€ feature_ablation.png            # Visualization referenced in reports
â”œâ”€â”€ baselines/                              # Baseline algorithm comparisons
â”‚   â”œâ”€â”€ baseline_comparison.png             # GNN vs Greedy vs Random
â”‚   â””â”€â”€ comparison_results.json             # Numerical results
â””â”€â”€ README.md                               # This file
```

Ablation checkpoints that accompany these figures are stored separately under
`checkpoints/ablations/architecture/<arch>/` to keep large binaries out of the
`results/` tree.

## ğŸ”„ Workflow

1. **Run experiment** â†’ outputs go to `checkpoints/<run_id>/evaluation/`
2. **Identify canonical results** â†’ copy specific files here
3. **Reference in docs** â†’ use paths like `results/baselines/figure.png`
4. **Version control** â†’ commit these files (they're small and important)

## ğŸ“Š Versioning Policy

- âœ… **DO version:** PNG plots, small JSON/CSV files (<1 MB)
- âŒ **DON'T version:** Large datasets, model weights (use Git LFS or cloud storage)
- ğŸ“ **Document origin:** Add comments or metadata about which run produced each file

## ğŸ”— See Also

- [Documentation Index](../docs/index.md) - All project documentation
- [Validation Report](../docs/reports/validation_report_2025-10-20.md) - Detailed experimental results
- [Execution Guide](../docs/guides/execution_guide.md) - How to reproduce results

---

**Philosophy:** This is the "source of truth" for publishable results. If it's in `results/`, it should be stable and reproducible.
